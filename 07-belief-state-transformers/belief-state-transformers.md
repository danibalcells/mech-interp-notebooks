# Notebook Implementation Instructions: Belief States in the Residual Stream

You are implementing a Jupyter Book notebook documenting a mechanistic interpretability research project. The notebook will live in the 07-belief-state-transformers folder inside the mech-interp-notebooks repo, just like the rest of the notebooks in the repo each live in their own folder.
The research code we will base the notebook on lives at ../belief-state-transformers/
Read the README.md and relevant source files from that repo before writing any notebook code.
When you need to understand how something works (data formats, checkpoint structure, class interfaces,
metric definitions), read the source code directly rather than guessing. If you encounter ambiguity
that the source code doesn't resolve, ask for clarification rather than inventing an answer.
If you need to access stored weights in the notebook, copy them over from the belief-state-transformers repo to the same folder as the notebook.

The notebook should be pedagogical — accessible to someone with ML basics but new to mechanistic
interpretability. Write explanatory prose between code cells. Avoid excessive jargon without definition. Most importantly, check the language and tone of the other notebooks in this book to make sure you're writing in the same style.

---

## Section 1 — The Setup and the Fractal

### 1a — Motivation

Prose: frame the core question. Transformer language models are trained on next-token prediction —
does that force them to track latent structure in the data? Introduce the tension between "stochastic
parrot" (surface statistics) and "world model" (latent structure). The claim being tested: if a model
achieves near-optimal prediction on sequences generated by a Hidden Markov Model, it must be
computing something close to the Bayesian posterior over hidden states — the belief state. Can we
find that belief state linearly encoded in the residual stream?

### 1b — The Mess3 HMM

Introduce Mess3: a 3-state, 3-token edge-emitting HMM from Shai et al. (2024). Explain what makes
it a good test case:
- The belief state (posterior over hidden states) and next-token probabilities are *distinct* — multiple
  distinct belief states yield the same next-token distribution. So finding the belief geometry in the
  residual stream is a strictly stronger result than just finding next-token probabilities.
- The belief geometry has an analytic ground truth (a fractal in the 2-simplex) to compare against.
- It has infinite Markov order despite only 3 hidden states, so no finite n-gram model can solve it.

**Live code cell:** Import `HMM.py` from the research repo. Instantiate the Mess3 HMM. Sample a
sequence of ~30 tokens. Print the token sequence and the corresponding belief state vectors
side-by-side (a small table is fine). Reference the relevant class/method from the source code.

**Animation cell:** Animate a single belief trajectory on the 2-simplex as tokens arrive, one frame
per token. Show the belief state as a moving dot; draw the simplex triangle as background. Use
matplotlib FuncAnimation or an ipywidgets slider. Keep it to ~50 tokens so it's readable.
Label each vertex A, B, C (the hidden states). This should take seconds to compute.

### 1c — The Fractal Geometry

Brief prose: explain why the belief states don't fill the simplex uniformly. The recursive
structure of Mess3's update equations produces a self-similar (fractal) attractor — each token
constrains the next belief to a specific sub-region of the simplex.

**Live code cell:** Generate a long sequence (~100k tokens, which is fast) and scatter all belief
states on the 2-simplex. Color points by sequence position or emitted token — check `HMM.py`
and `utils/plotting.py` / `utils/simplex.py` for the projection logic already implemented.

**Interactive cell:** Add a slider that controls how many tokens are included in the scatter plot
(try powers of 10: 100, 1k, 10k, 100k). The fractal structure filling in as the slider increases
is the key visual payoff. Reference `images/beliefs_by_position_dataset_1000000.png` as the
asymptotic case — display it inline as a reference figure.

---

## Section 2 — The Transformer

### Prose

Describe the architecture using the table from the README. Add one key observation beyond the
table: the model is small enough (d_model=64, 1 head, context length 10) that it almost certainly
cannot memorize sequences — it has to learn a *general* solution. This makes it a meaningful test
of whether the model has internalized the HMM structure rather than cached sequence-specific answers.

Training objective: next-token prediction with cross-entropy loss on sequences sampled fresh each
batch from the Mess3 HMM.

### Load weights

Include a commented-out code cell showing the full training command from the README with a note
that it runs for 1M epochs (~24 hours to full convergence; a good checkpoint exists at much less).
Then load the pretrained AdamW checkpoint from `final_checkpoints/transformer/adamw/`.

Check `transformer.py` to understand how to load and call `BeliefStateTransformer`. Make sure the
checkpoint loading matches the class interface — do not assume, read the source.

### Training dynamics animation

Display `images/adamw_pca_multi.png` inline. This shows PCA of the residual stream activations
at periodic training checkpoints — the fractal geometry emerging over the course of training.
Write a brief explanation of what the reader is seeing: early in training the activations are
unstructured; over time a geometry consistent with the ground-truth fractal belief simplex appears.

If multiple intermediate checkpoints are available in `final_checkpoints/` (check what's actually
saved before writing this cell), add a slider to replay the evolution interactively. If only the
final checkpoint is available, show the static multi-panel image and note that the animation is
possible if intermediate checkpoints are saved.

---

## Section 3 — Linear Probing

### Prose

Explain the method: fit a linear map from 64-dim residual stream activations → 3D belief state
vector, using standard linear regression (MSE loss). Check `probes/linear.py` to confirm the
exact implementation and loss.

Address temporal alignment carefully — this is subtle and important. In an edge-emitting HMM,
the token at position t is emitted *during* the transition, so the belief state that should be
predicted from the activation at position t is the posterior *after* observing token t. Explain
which indexing convention the code uses (check `scripts/sample_acts.py` and `scripts/train_probe.py`
to confirm the exact pairing of activation index and belief index — do not assume).

### Activations

Explain that the dataset consists of `(activation, belief)` pairs pre-sampled by running the
trained transformer on HMM sequences. Check `scripts/sample_acts.py` to understand the sampling
process: how sequences are generated, which layer's activations are saved, and what the output
format looks like.

If sampling a fresh activation dataset at notebook runtime is fast (estimate the time by checking
sequence count and model size — the transformer is tiny), do it live with a clearly labeled cell
and a progress bar. If it would take more than ~2 minutes, load a pre-saved `dataset.pt` from
`outputs/` or from wherever the README says outputs are stored. Either way, include the sampling
code (live or commented) so the reader understands the pipeline.

### Probe training

**Live code cell:** Train the linear probe on the activation dataset. This should be fast (linear
regression on a small dataset). Reference `probes/linear.py` and `scripts/train_probe.py` for
the training logic. Show a loss curve.

**Important:** Make sure the probe is trained on activations from the *same checkpoint* that was
loaded in Section 2. If loading a pre-trained probe from disk, verify it matches the model
checkpoint. Ask for clarification if the checkpoint/probe correspondence isn't clear from the
directory structure.

**Animation:** While the probe trains (or replaying saved probe snapshots if you save them during
training), animate the probe's output on the 2-simplex. Start from random/unstructured projections
and show the geometry converging toward the fractal. Even a handful of snapshots (5–10 across
training) makes this compelling. If probe training is too fast to meaningfully animate live,
save periodic snapshots during training for the animation.

### Visualization

Side-by-side simplex comparison: ground truth optimal beliefs (left) vs. probe-predicted beliefs
(right), both projected to 2D. Reference `images/comparison-20260204_182437.png` as the target.
Then show the four 3D views from `images/probe_3d_views-20260203_121403.png`.
Check `scripts/plot_belief_probe_comparison.py` and `scripts/plot_probe_3d_four_views.py` for
the plotting logic to reuse.

---

## Section 4 — Causal Intervention

### 4a — Motivation

Prose: correlation vs. causation. A linear probe shows the geometry is *present* in the residual
stream, but not that the model *uses* it for prediction. To test causality, we need to intervene:
replace or modify the encoded belief and check whether predictions shift as expected.

The prediction: if we steer the residual stream toward the belief state corresponding to a
*counterfactual* token history, then:
- KL(optimal-actual ‖ steered-prediction) should *increase* — the model predicts less like what's
  optimal for the real sequence
- KL(optimal-counterfactual ‖ steered-prediction) should *decrease* — the model predicts more
  like what's optimal for the counterfactual

### 4b — The autoencoder

Prose: why not just subtract the probe direction? The 3D probe directions span a non-orthogonal
subspace of the 64-dim residual stream, so inverting the probe to construct a target activation
is not well-defined. The autoencoder solves this by learning an invertible mapping.

Describe the dual-loss architecture: reconstruction loss (MSE between decoded activation and
original) keeps the decoder invertible; geometry loss (MSE between encoded latent and projected
belief) aligns the 2D latent space with the simplex projection. Check `probes/autoencoder.py`
and `scripts/train_autoencoder.py` for the exact loss terms and λ hyperparameters.

Include a brief honest note: the λ tradeoff between the two losses required iteration. If there
are any saved visualizations or notes showing what happens when one loss dominates, include them.

### 4c — Steering

Describe the three belief sources (counterfactual, other_seq_reachable, random_simplex) and the
two intervention modes (replace, additive). Check `interventions/steering.py` for exact implementation.

Walk through the counterfactual setup in concrete terms: given a sequence ...X_t-1, X_t..., the
counterfactual asks "what would the belief be at position t if X_t-1 had been Y instead?" Clarify
which position the counterfactual is computed at and how the counterfactual belief is obtained —
read the source rather than assuming.

**Interactive single-example cell:** Load the autoencoder and a steering intervention. Pick one
sequence, apply counterfactual steering at one position, and print the before/after logits and
the before/after KL values. This makes the mechanism tangible at the single-example level before
showing aggregates.

**Load aggregate results:** Load pre-computed results from `outputs/` (check the README pipeline
for where `run_steering_experiment.py` saves them). Display `images/steering_summary_steering_20260206_113903.png`
and explain the crossover pattern clearly: each paired line connects the same sequence's KL under
no-steering vs. steering; red lines are KL to optimal-actual, blue to optimal-counterfactual.
The crossing is the causal signature.

Reference `scripts/plot_steering_summary.py` and `scripts/plot_steering_from_results.py` for
the plotting logic.

**Additive sweep animation:** Display the additive sweep result from
`images/additive_sweep_20260205_113420_unique.png`. If the sweep results are saved to disk,
add an interactive slider over λ showing the prediction distribution (or the KL values) morphing
continuously from 0→1. Check `scripts/plot_additive_steering_from_results.py` for the data format.

---

## Section 5 — Discussion

Brief prose:
- What this demonstrates: belief states are not just present in the residual stream but causally
  upstream of next-token prediction. The geometry is functional, not epiphenomenal.
- What remains open: does the transformer implement the *dynamics* (the Bayesian update step) or
  just store the static geometry? Does this hold in larger models? Your team (Simplex) is pursuing
  the production-scale version.
- Mention the zero-ablation baseline (`interventions/zero_ablation.py`) briefly: replacing the
  residual stream with zeros degrades predictions uniformly, in contrast to the structured shift
  from counterfactual steering. This rules out that the steering effect is just "any perturbation
  moves predictions."

---

## General instructions for the coding agent

- Read source files before writing code. Do not invent class interfaces, method signatures, or
  data formats — check the actual files in the repo.
- For any step where you're uncertain about the correct checkpoint path, dataset path, or
  pairing between model and probe checkpoints, ask rather than guessing.
- Reuse existing plotting utilities from `utils/plotting.py` and `utils/simplex.py` where relevant.
- All animations should be self-contained and run without requiring GPU — the model is small
  enough to run on CPU for inference.
- Keep prose cells concise. The target reader has ML basics but is new to mechanistic
  interpretability. Define terms on first use; don't assume familiarity with computational
  mechanics or HMMs beyond what the notebook itself introduces.
- If a computation would take more than ~2 minutes, load pre-saved results and include the
  generation code in a commented cell.